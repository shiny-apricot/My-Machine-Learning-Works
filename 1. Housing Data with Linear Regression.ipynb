{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# % % markdown\n",
        "# # # Index\n",
        "# % % markdown\n",
        "#[1. Correlation Matrix](#corr) < br >\n",
        "  #[2. Stratified Shuffle Split](#split) < br >\n",
        "  #[3. Plotting the Data](#plot) < br >\n",
        "  #[4. Pipeline Implementation](#pipeline) < br >\n",
        "  #[5. Column Tranformer](#column_transformer) < br >\n",
        "  #[6. Random Forest Regressor](#random_forest) < br >\n",
        "  # $\\;\\;\\;\\;\\;\\;\n",
        "$[6.1 Train Set Prediction](#train_predict) < br >\n",
        "  # $\\;\\;\\;\\;\\;\\;\n",
        "$[6.2 Test Set Prediction](#test_predict) < br > < br >\n",
        "  # % % markdown\n",
        "# < blockquote > < b > Yasin Ä°nal < /b>\n",
        "# These are the notes that I 've taken from the book '\n",
        "Hands - On Machine Learning with Scikit - Learn, Keras & Tensorflow ' by Aurelion Geron </blockquote>\n",
        "\n",
        "# % % codecell\n",
        "import os\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
        "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
        "\n",
        "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
        "\n",
        "def fetch_housing_data(housing_url = HOUSING_URL, housing_path = HOUSING_PATH):\n",
        "  os.makedirs(housing_path, exist_ok = True)\n",
        "tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "housing_tgz = tarfile.open(tgz_path)\n",
        "housing_tgz.extractall(path = housing_path)\n",
        "housing_tgz.close()\n",
        "\n",
        "# % % codecell\n",
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path = HOUSING_PATH):\n",
        "  csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "return pd.read_csv(csv_path)\n",
        "\n",
        "# % % codecell\n",
        "fetch_housing_data()\n",
        "housing = load_housing_data()\n",
        "housing.head()\n",
        "# % % markdown\n",
        "# < a id = \"corr\" > < /a>\n",
        "# # # # # Correlation Matrix\n",
        "\n",
        "\n",
        "# % % codecell\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending = False)\n",
        "# % % markdown\n",
        "# < a id = \"plot\" > < /a>\n",
        "# # # # Plotting the Data\n",
        "\n",
        "# % % codecell %\n",
        "  matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "housing.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", alpha = 0.4,\n",
        "  s = housing[\"population\"] / 100, label = \"population\", figsize = (10, 7),\n",
        "  c = \"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar = True, )\n",
        "\n",
        "plt.legend()\n",
        "# % % markdown\n",
        "# < a id = \"split\" > < /a>\n",
        "# # # # Stratified Shuffle Split\n",
        "\n",
        "# % % codecell\n",
        "import numpy as np\n",
        "\n",
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "  bins = [0., 1.5, 3., 4.5, 6., np.inf],\n",
        "  labels = [1, 2, 3, 4, 5])\n",
        "\n",
        "# % % codecell\n",
        "from sklearn.model_selection\n",
        "import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "  strat_train_set = housing.loc[train_index]\n",
        "strat_test_set = housing.loc[test_index]\n",
        "\n",
        "# % % codecell\n",
        "strat_test_distribute = strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
        "strat_train_distribute = strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)\n",
        "strat_test_distribute, strat_train_distribute\n",
        "# % % markdown\n",
        "# < code > The data of each train and data set are distributed very fairly and equally.\n",
        "\n",
        "# % % codecell\n",
        "for set_ in (strat_train_set, strat_test_set):\n",
        "  set_.drop(\"income_cat\", axis = 1, inplace = True)\n",
        "\n",
        "# % % codecell\n",
        "train_feed = strat_train_set.drop(\"median_house_value\", axis = 1)\n",
        "test_feed = strat_test_set.drop(\"median_house_value\", axis = 1)\n",
        "\n",
        "# % % codecell\n",
        "train_labels = strat_train_set[\"median_house_value\"].copy()\n",
        "test_labels = strat_test_set[\"median_house_value\"].copy()\n",
        "\n",
        "# % % codecell\n",
        "len(train_feed), len(test_feed)\n",
        "# % % markdown\n",
        "# < a id = \"pipeline\" > < /a>\n",
        "# # # # Pipeline\n",
        "\n",
        "# % % codecell\n",
        "from sklearn.pipeline\n",
        "import Pipeline\n",
        "from sklearn.preprocessing\n",
        "import StandardScaler\n",
        "from sklearn.impute\n",
        "import SimpleImputer\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "  ('imputer', SimpleImputer(strategy = \"median\")),\n",
        "  ('std_scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "\n",
        "# % % codecell\n",
        "train_feed['ocean_proximity']\n",
        "# % % markdown\n",
        "# < blockquote > Ocean Proximity is not a numerical feature as you can see. < br >\n",
        "  # I will transform it by using OneHotEncoder < /blockquote>\n",
        "\n",
        "# % % codecell\n",
        "housing_num = train_feed.drop(\"ocean_proximity\", axis = 1)\n",
        "# % % markdown\n",
        "# < a id = \"column_transformer\" > < /a>\n",
        "# # # # Column Transformer\n",
        "\n",
        "# % % codecell\n",
        "from sklearn.compose\n",
        "import ColumnTransformer\n",
        "from sklearn.preprocessing\n",
        "import OneHotEncoder\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "  (\"num\", num_pipeline, num_attribs),\n",
        "  (\"cat\", OneHotEncoder(), cat_attribs)\n",
        "])\n",
        "\n",
        "train_prepared = full_pipeline.fit_transform(train_feed)\n",
        "test_prepared = full_pipeline.fit_transform(test_feed)\n",
        "# % % markdown\n",
        "# < a id = \"random_forest\" > < /a>\n",
        "# # # # Random Forest Regressor\n",
        "\n",
        "# % % codecell\n",
        "from sklearn.ensemble\n",
        "import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(train_prepared, train_labels)\n",
        "housing_predictions_train = forest_reg.predict(train_prepared)\n",
        "\n",
        "# % % codecell\n",
        "from sklearn.metrics\n",
        "import mean_squared_error\n",
        "from sklearn.model_selection\n",
        "import cross_val_score\n",
        "\n",
        "forest_scores_tr = cross_val_score(forest_reg, train_prepared, train_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n",
        "forest_rmse_scores_tr = np.sqrt(-forest_scores)\n",
        "\n",
        "# % % codecell\n",
        "def display_scores(scores):\n",
        "  print(\"Mean:\", scores.mean())\n",
        "print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "# % % codecell\n",
        "display_scores(forest_rmse_scores_tr)\n",
        "# % % markdown\n",
        "# < a id = \"train_predict\" > < /a>\n",
        "# # # Train Set Prediction\n",
        "\n",
        "# % % codecell\n",
        "display_scores(forest_rmse_scores), forest_rmse\n",
        "# % % markdown\n",
        "# < a id = \"test_predict\" > < /a>\n",
        "# # # Test Set Prediction\n",
        "\n",
        "# % % codecell\n",
        "housing_predictions_test = forest_reg.predict(test_prepared)\n",
        "\n",
        "# % % codecell\n",
        "forest_rmse = mean_squared_error(test_labels, housing_predictions_test, squared = False)\n",
        "forest_scores = cross_val_score(forest_reg, test_prepared, test_labels, scoring = \"neg_mean_squared_error\", cv = 10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "\n",
        "# % % codecell\n",
        "display_scores(forest_rmse_scores), forest_rmse\n",
        "# % % markdown\n",
        "# < br >\n",
        "  # < blockquote > To understand the results better,\n",
        "  let 's examine the <code>median_house_value</code> in detail.\n",
        "\n",
        "# % % codecell\n",
        "housing['median_house_value'].describe()\n",
        "# % % markdown\n",
        "# < blockquote > Since the standard deviation of the 'median_house_value'\n",
        "is < code > 115, 395 < /code> , the prediction error of <code>53,000</code > is moderately good.\n",
        "\n",
        "# % % codecell\n",
        "#\n",
        "import dill\n",
        "# dill.dump_session('housing.db')\n",
        "# dill.load_session('housing.db')\n",
        "\n",
        "# % % codecell"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}